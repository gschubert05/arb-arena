name: Scrape (FAST + notify)

on:
  workflow_dispatch:
  schedule:
    - cron: "2-59/5 * * * *"   # every 5 minutes, starting at :02 UTC

permissions:
  contents: write

concurrency:
  group: scrape-arena
  cancel-in-progress: true

env:
  ROI_THRESHOLD_PCT: "2.0"
  NOTIFY_BOOKIES: "sportsbet,bet365,neds,tab"
  PUBLISH_JSON_PATH: "server/data/opportunities.json"
  SEEN_KEYS_PATH: "server/data/seen_keys.json"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # we'll read origin/data without switching branches

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Chrome + matching ChromeDriver
      - id: setup-chrome
        uses: browser-actions/setup-chrome@v2
        with:
          chrome-version: stable
          install-dependencies: true
          install-chromedriver: true

      - name: Export Chrome paths
        run: |
          echo "CHROME_BIN=${{ steps.setup-chrome.outputs.chrome-path }}" >> $GITHUB_ENV
          echo "CHROMEDRIVER_PATH=${{ steps.setup-chrome.outputs.chromedriver-path }}" >> $GITHUB_ENV

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 lxml

      - name: Install Xvfb
        run: sudo apt-get update && sudo apt-get install -y xvfb

      - uses: actions/checkout@v4

      # Also checkout the `data` branch into a subfolder
      - name: Checkout data branch
        uses: actions/checkout@v4
        with:
          ref: data
          path: data-branch

      # Parse active_comp_ids.json from that folder and export COMP_IDS + ACTIVE_JSON_URL
      - name: Load active competition IDs (from data branch folder)
        shell: bash
        run: |
          set -euo pipefail
          JSON_PATH="data-branch/server/data/active_comp_ids.json"

          if [[ -f "$JSON_PATH" ]]; then
            COMP_CSV=$(python - <<'PY'
            import json, sys
            p = "data-branch/server/data/active_comp_ids.json"
            with open(p, "r", encoding="utf-8") as f:
                d = json.load(f)
            if isinstance(d, dict) and "active_comp_ids" in d:
                ids = d["active_comp_ids"]
            elif isinstance(d, list):
                ids = d
            else:
                ids = []
            print(",".join(str(x) for x in ids))
            PY
            )
            if [[ -n "$COMP_CSV" ]]; then
              echo "COMP_IDS=$COMP_CSV" >> "$GITHUB_ENV"
              echo "ACTIVE_JSON_URL=https://raw.githubusercontent.com/${GITHUB_REPOSITORY}/data/server/data/active_comp_ids.json" >> "$GITHUB_ENV"
              echo "Using COMP_IDS=$COMP_CSV"
            else
              echo "Parsed empty active_comp_ids.json; falling back"
              echo "COMP_IDS=10,11" >> "$GITHUB_ENV"
            fi
          else
            echo "File not found: $JSON_PATH ; falling back"
            echo "COMP_IDS=10,11" >> "$GITHUB_ENV"
          fi

      - name: Test DNS / curl
        run: curl -v http://odds.aussportsbetting.com/multibet

      - name: Run scraper under Xvfb
        env:
          COMP_IDS: ${{ env.COMP_IDS }}
          FORCE_HEADLESS: "true"
        run: |
          nohup Xvfb :99 -screen 0 1280x1024x24 >/tmp/xvfb.log 2>&1 &
          export DISPLAY=:99
          python scraper/scraper.py

      # ✅ Load previous state from origin/data without switching branches
      - name: Load previous state (for notifications)
        run: |
          git fetch origin data || true
          if git show origin/data:server/data/opportunities.json > prev.json 2>/dev/null; then
            echo "Loaded previous opportunities."
          else
            echo '{}' > prev.json
          fi
          if git show origin/data:server/data/seen_keys.json > seen_keys.json 2>/dev/null; then
            echo "Loaded seen keys."
          else
            echo '[]' > seen_keys.json
          fi

      - name: Notify about new arbs (Telegram/Discord)
        env:
          ROI_THRESHOLD_PCT:  ${{ env.ROI_THRESHOLD_PCT }}
          NOTIFY_BOOKIES:     ${{ env.NOTIFY_BOOKIES }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID:   ${{ secrets.TELEGRAM_CHAT_ID }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          python scripts/notify.py \
            --input "server/data/opportunities.json" \
            --seen "seen_keys.json" \
            --roi-threshold-pct "${ROI_THRESHOLD_PCT}" \
            --notify-bookies "${NOTIFY_BOOKIES}"

      - name: Place seen_keys.json where we commit it from
        run: |
          mkdir -p server/data
          test -f seen_keys.json || echo "[]" > seen_keys.json
          mv -f seen_keys.json server/data/seen_keys.json

      # ✅ Publish ONLY opportunities + seen_keys via a separate worktree (no conflicts, no force)
      - name: Publish JSON + seen_keys to data branch
        run: |
          set -e
          git config user.name "github-actions"
          git config user.email "actions@github.com"

          git fetch origin data || true

          # Use a temporary worktree for the data branch
          WT_DIR="$(mktemp -d)"
          if git rev-parse --verify origin/data >/dev/null 2>&1; then
            git worktree add -B data "$WT_DIR" origin/data
          else
            git worktree add -B data "$WT_DIR"
          fi

          mkdir -p "$WT_DIR/server/data"
          cp -f server/data/opportunities.json "$WT_DIR/server/data/opportunities.json"
          cp -f server/data/seen_keys.json "$WT_DIR/server/data/seen_keys.json"

          cd "$WT_DIR"
          git add -f server/data/opportunities.json
          git add -f server/data/seen_keys.json
          git commit -m "fast: data $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push origin data

          # Clean up the worktree
          cd -
          git worktree remove "$WT_DIR" --force
