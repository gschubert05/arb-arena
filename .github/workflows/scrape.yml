name: Scrape (FAST + notify)

on:
  workflow_dispatch:
  schedule:
    - cron: "2-59/5 * * * *"   # every 5 minutes, starting at :02 UTC

permissions:
  contents: write

concurrency:
  group: scrape-arena
  cancel-in-progress: true

env:
  ROI_THRESHOLD_PCT: "2.0"
  PUBLISH_JSON_PATH: "server/data/opportunities.json"
  SEEN_KEYS_PATH: "server/data/seen_keys.json"
  RAW_JSON_URL: "https://raw.githubusercontent.com/${{ github.repository }}/data/server/data/opportunities.json"
  RAW_SEEN_URL: "https://raw.githubusercontent.com/${{ github.repository }}/data/server/data/seen_keys.json"
  RAW_ACTIVE_URL: "https://raw.githubusercontent.com/${{ github.repository }}/data/server/data/active_comp_ids.json"
  NOTIFY_BOOKIES: "sportsbet,bet365,neds,tab"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Install Chrome + Chromedriver
      - name: Set up Chrome
        id: setup-chrome
        uses: browser-actions/setup-chrome@v2
        with:
          chrome-version: stable
          install-dependencies: true
          install-chromedriver: true

      # Export Chrome paths
      - name: Export Chrome env vars
        run: |
          echo "CHROME_BIN=${{ steps.setup-chrome.outputs.chrome-path }}" >> $GITHUB_ENV
          echo "CHROMEDRIVER_PATH=${{ steps.setup-chrome.outputs.chromedriver-path }}" >> $GITHUB_ENV

      # Install Python dependencies
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      # Load active competition IDs (fallback to defaults)
      - name: Load active competition IDs
        run: |
          mkdir -p server/data
          curl -fsSL "$RAW_ACTIVE_URL" -o server/data/active_comp_ids.json || echo '{"active_comp_ids":[]}' > server/data/active_comp_ids.json
          COMP_IDS=$(python - <<'PY'
          import json
          try:
              d=json.load(open("server/data/active_comp_ids.json","r",encoding="utf-8"))
              ids=d.get("active_comp_ids",[])
              print(",".join(str(x) for x in ids))
          except Exception:
              print("")
          PY
          )
          echo "COMP_IDS=${COMP_IDS}" >> $GITHUB_ENV
          echo "Loaded COMP_IDS=${COMP_IDS}"

      # Run scraper under Xvfb (non-headless)
      - name: Run scraper (FAST set)
        env:
          COMP_IDS: "${{ env.COMP_IDS || '10,11,40,41' }}"
          FORCE_HEADLESS: "false"
        run: |
          sudo apt-get update && sudo apt-get install -y xvfb
          nohup Xvfb :99 -screen 0 2560x1440x24 >/tmp/xvfb.log 2>&1 &
          export DISPLAY=:99
          python scraper/scraper.py

      # Load previous state
      - name: Load previous state
        run: |
          curl -fsSL "$RAW_JSON_URL" -o prev.json || echo "{}" > prev.json
          curl -fsSL "$RAW_SEEN_URL" -o seen_keys.json || echo "[]" > seen_keys.json

      # Compute new arbs and notify
      - name: Compute new arbs and notify
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID:   ${{ secrets.TELEGRAM_CHAT_ID }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          ROI_THRESHOLD_PCT:  ${{ env.ROI_THRESHOLD_PCT }}
          NOTIFY_BOOKIES:     ${{ env.NOTIFY_BOOKIES }}
        run: python scripts/notify.py  # (your long inline Python should live in scraper/scripts/notify.py)

      # Upload debug artifacts
      - name: Upload debug artifacts (if any)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-debug
          path: |
            server/data/debug_comp_*.html
            server/data/debug_comp_*.png
            server/data/debug_comp_*_curl.txt
            server/data/debug_env.txt

      # Stage seen_keys for commit
      - name: Place seen_keys.json where we commit it from
        run: |
          mkdir -p server/data
          test -f seen_keys.json || echo "[]" > seen_keys.json
          mv -f seen_keys.json server/data/seen_keys.json

      # Publish JSON + seen_keys (and active IDs) to data branch
      - name: Publish data to branch
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git fetch origin
          git checkout -B data
          mkdir -p server/data
          git add -f "$PUBLISH_JSON_PATH"
          git add -f "$SEEN_KEYS_PATH"
          test -f server/data/active_comp_ids.json && git add -f server/data/active_comp_ids.json || true
          git commit -m "fast: data $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push origin data --force
