name: Scrape (FAST + notify)

on:
  workflow_dispatch:
  schedule:
    - cron: "2-59/5 * * * *"   # every 5 minutes, starting at :02 UTC

permissions:
  contents: write

concurrency:
  group: scrape-arena
  cancel-in-progress: true

env:
  # set your ROI threshold (percent). Example: 2.0 means alert when roi >= 2%
  ROI_THRESHOLD_PCT: "2.0"
  # path we publish on the data branch
  PUBLISH_JSON_PATH: "server/data/opportunities.json"
  # where we persist seen keys on the data branch
  SEEN_KEYS_PATH: "server/data/seen_keys.json"
  # raw URLs to read previous state
  RAW_JSON_URL: "https://raw.githubusercontent.com/${{ github.repository }}/data/server/data/opportunities.json"
  RAW_SEEN_URL: "https://raw.githubusercontent.com/${{ github.repository }}/data/server/data/seen_keys.json"
  # active competition ids, written daily by the discover workflow
  RAW_ACTIVE_URL: "https://raw.githubusercontent.com/${{ github.repository }}/data/server/data/active_comp_ids.json"

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Chrome for Testing + matching ChromeDriver (no apt flakiness)
      - id: setup-chrome
        uses: browser-actions/setup-chrome@v2
        with:
          chrome-version: stable
          install-dependencies: true
          install-chromedriver: true

      - name: Export CHROME_BIN
        run: echo "CHROME_BIN=${{ steps.setup-chrome.outputs.chrome-path }}" >> $GITHUB_ENV

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      # Load previously discovered active comp IDs (from data branch)
      - name: Load active competition IDs
        run: |
          mkdir -p server/data
          curl -fsSL "$RAW_ACTIVE_URL" -o server/data/active_comp_ids.json || echo '{"active_comp_ids":[]}' > server/data/active_comp_ids.json
          COMP_IDS=$(python - <<'PY'
          import json
          try:
              d=json.load(open("server/data/active_comp_ids.json","r",encoding="utf-8"))
              ids=d.get("active_comp_ids",[])
              print(",".join(str(x) for x in ids))
          except Exception:
              print("")
          PY
          )
          echo "COMP_IDS=${COMP_IDS}" >> $GITHUB_ENV
          echo "Loaded COMP_IDS=${COMP_IDS}"

      # Run your scraper (enriches with book_table.best)
      - name: Run scraper (FAST set)
        env:
          # Prefer discovered IDs; else use repo variable; else fallback
          COMP_IDS: "${{ env.COMP_IDS || vars.COMP_IDS || '10,11,40,41' }}"
        run: python scraper/scraper.py

      # Pull the last published JSON + seen keys from 'data' branch (ignore errors if first run)
      - name: Load previous state
        run: |
          curl -fsSL "$RAW_JSON_URL" -o prev.json || echo "{}" > prev.json
          curl -fsSL "$RAW_SEEN_URL" -o seen_keys.json || echo "[]" > seen_keys.json

      # ✅ New: Compact Telegram/Discord message with JUST bookies + odds + ROI (no URLs)
      - name: Compute new arbs and notify (bookies, odds, ROI only)
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID:   ${{ secrets.TELEGRAM_CHAT_ID }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          ROI_THRESHOLD_PCT:  ${{ env.ROI_THRESHOLD_PCT }}
        run: |
          python - << 'PY'
          import json, os, subprocess

          ROI_THRESHOLD_PCT = float(os.environ.get("ROI_THRESHOLD_PCT","2.0"))
          THRESH = ROI_THRESHOLD_PCT / 100.0

          def load_json(path, default):
              try:
                  with open(path, "r", encoding="utf-8") as f:
                      return json.load(f)
              except Exception:
                  return default

          cur = load_json("server/data/opportunities.json", {"items":[]})
          if isinstance(cur, list):
              cur = {"items": cur}
          items = cur.get("items", [])

          seen = load_json("seen_keys.json", [])
          seen_set = set(seen if isinstance(seen, list) else [])

          def key(it):
              def n(x): return (x or "").strip().lower()
              return "|".join([
                  str(it.get("competitionid") or it.get("competitionId") or ""),
                  n(it.get("sport")), n(it.get("game")),
                  n(it.get("market")), n(it.get("match")),
                  it.get("dateISO") or it.get("date") or ""
              ])

          new_hits = []
          for it in items:
              try:
                  roi = float(it.get("roi") or 0.0)
              except:
                  roi = 0.0
              if roi < THRESH:
                  continue

              best = (it.get("book_table") or {}).get("best") or {}
              L = best.get("left")  or {}
              R = best.get("right") or {}
              # Need agencies AND odds to format the compact message
              if not (L.get("agency") and R.get("agency") and L.get("odds") and R.get("odds")):
                  continue

              k = key(it)
              if k in seen_set:
                  continue
              new_hits.append(it)
              seen_set.add(k)

          # Save updated seen_keys
          with open("seen_keys.json", "w", encoding="utf-8") as f:
              json.dump(sorted(list(seen_set)), f, ensure_ascii=False, indent=0)

          if not new_hits:
              raise SystemExit(0)

          new_hits.sort(key=lambda x: float(x.get('roi') or 0), reverse=True)

          # Build compact lines: "<BookieA> @ X.XX\n<BookieB> @ Y.YY\nROI: Z.ZZ%"
          def fmt(it):
              best = it["book_table"]["best"]
              L, R = best["left"], best["right"]
              roi_pct = f"{(float(it.get('roi') or 0)*100):.2f}%"
              return f"{L['agency']} @ {float(L['odds']):.2f}\n{R['agency']} @ {float(R['odds']):.2f}\nROI: {roi_pct}"

          lines = [fmt(it) for it in new_hits[:8]]
          msg = "New arbs ≥ " + str(ROI_THRESHOLD_PCT) + "%\n\n" + "\n\n".join(lines)
          open("msg.txt","w",encoding="utf-8").write(msg)

          # Telegram
          tok = os.environ.get("TELEGRAM_BOT_TOKEN")
          chat= os.environ.get("TELEGRAM_CHAT_ID")
          if tok and chat:
              api = f"https://api.telegram.org/bot{tok}/sendMessage"
              subprocess.run([
                  "curl","-fsS","-X","POST",api,
                  "-d", f"chat_id={chat}",
                  "--data-urlencode","disable_web_page_preview=true",
                  "--data-urlencode", f"text={msg}"
              ], check=False)

          # Discord (optional)
          wh = os.environ.get("DISCORD_WEBHOOK_URL")
          if wh:
              payload = json.dumps({"content": msg})
              subprocess.run([
                  "curl","-fsS","-H","Content-Type: application/json",
                  "-d", payload, wh
              ], check=False)
          PY

      - name: Place seen_keys.json where we commit it from
        run: |
          mkdir -p server/data
          test -f seen_keys.json || echo "[]" > seen_keys.json
          mv -f seen_keys.json server/data/seen_keys.json

      # Publish JSON + seen_keys (and active IDs if present) to data branch
      - name: Publish JSON + seen_keys to data branch
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git fetch origin
          git checkout -B data
          mkdir -p server/data
          git add -f "$PUBLISH_JSON_PATH"
          git add -f "$SEEN_KEYS_PATH"
          test -f server/data/active_comp_ids.json && git add -f server/data/active_comp_ids.json || true
          git commit -m "fast: data $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push origin data --force
