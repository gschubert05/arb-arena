name: Discover active comp IDs (daily)

on:
  workflow_dispatch:
  schedule:
    - cron: "12 0 * * *"  # daily at 00:12 UTC

permissions:
  contents: write

jobs:
  discover:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Install Python deps your discover script needs
      - name: Install Python deps (selenium + bs4)
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4

      # Set up Chrome + matching ChromeDriver (same as your main scraper.yml)
      - id: setup-chrome
        uses: browser-actions/setup-chrome@v2
        with:
          chrome-version: stable
          install-dependencies: true
          install-chromedriver: true
      - name: Export CHROME_BIN
        run: echo "CHROME_BIN=${{ steps.setup-chrome.outputs.chrome-path }}" >> $GITHUB_ENV

      # Run your Selenium-based discover script (headless by default)
      - name: Run discoverer (headless)
        run: |
          mkdir -p server/data
          python scraper/discover_active_compids.py \
            --range "1-150" --skip "72,73,108,114" \
            --out server/data/active_comp_ids.json -v

      # Publish to data branch so scraper.yml can load it
      - name: Publish active_comp_ids.json to data branch
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git fetch origin
          git checkout -B data
          mkdir -p server/data
          git add -f server/data/active_comp_ids.json
          git commit -m "discover: active comp IDs $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "No changes"
          git push origin data --force
